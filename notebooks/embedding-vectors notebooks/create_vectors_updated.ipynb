{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllabus Vectors\n",
    "\n",
    "This notebook will attempt to recreate what was done for the Web of Science data with the institution and city vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecoder, JSONDecodeError\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "The data has no real information so let us first get some meta data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, strip_numeric, strip_non_alphanum, stem_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_WHITESPACE = re.compile(r'[^\\s]')\n",
    "def decode_stacked(document, pos=0, decoder=JSONDecoder()):\n",
    "    while True:\n",
    "        match = NOT_WHITESPACE.search(document, pos)\n",
    "        if not match:\n",
    "            return\n",
    "        pos = match.start()\n",
    "\n",
    "        try:\n",
    "            obj, pos = decoder.raw_decode(document, pos)\n",
    "        except JSONDecodeError:\n",
    "            # do something sensible if there's some error\n",
    "            raise\n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_texts_cities = {}\n",
    "teaching_texts_orgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"openSyReal/\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(\"openSyReal/\" + filename, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                for jsonfile in decode_stacked(line):\n",
    "                    try:\n",
    "                        if jsonfile['grid_country_code'] == \"US\" and int(jsonfile['year']) > 2006:\n",
    "                            try:\n",
    "                                text = remove_stopwords(strip_numeric(strip_non_alphanum(jsonfile['text'].replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\xa0\", \" \").replace(\"\\t\", \"\").lower())))\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                            i += 1\n",
    "                            try:\n",
    "                                name = jsonfile['NAME']\n",
    "                                if name is not None and name not in teaching_texts_orgs:\n",
    "                                    teaching_texts_orgs[name] = []\n",
    "                                    teaching_texts_orgs[name].append(text)\n",
    "                                else:\n",
    "                                    teaching_texts_orgs[name].append(text)\n",
    "                            except KeyError:\n",
    "                                pass\n",
    "                            try:\n",
    "                                city = jsonfile['CITY']\n",
    "                                if name is not None and name not in teaching_texts_cities:\n",
    "                                    teaching_texts_cities[name] = []\n",
    "                                    teaching_texts_cities[name].append(text)\n",
    "                                else:\n",
    "                                    teaching_texts_cities[name].append(text)\n",
    "                            except KeyError:\n",
    "                                pass\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import TfidfModel\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(document, model, method):\n",
    "    if method == \"doc2vec\":\n",
    "        vector = model.infer_vector(document)\n",
    "        return vector\n",
    "    if method == \"word2vec\":\n",
    "        vector = model[document]\n",
    "        return vector\n",
    "    if method == \"tfidf\":\n",
    "        vector = model[document]\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel = Doc2Vec.load(\"alldoc2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now aggregate city wise and institution wise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_vector(texts, model, method):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        vector = create_vector(text, model, method)\n",
    "        vectors.append(vector)\n",
    "    # simple centroid, can do more complex method\n",
    "    return (np.mean(vectors, axis=0), np.var(vectors, axis=0), len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WoS Data\n",
    "\n",
    "We're going to now load up and use the WoS data and compare our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2006 = pd.read_csv(\"data_files_USA/data_2006_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2007 = pd.read_csv(\"data_files_USA/data_2007_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2008 = pd.read_csv(\"data_files_USA/data_2008_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2009 = pd.read_csv(\"data_files_USA/data_2009_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2010 = pd.read_csv(\"data_files_USA/data_2010_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2011 = pd.read_csv(\"data_files_USA/data_2011_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2012 = pd.read_csv(\"data_files_USA/data_2012_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2013 = pd.read_csv(\"data_files_USA/data_2013_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2014 = pd.read_csv(\"data_files_USA/data_2014_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2015 = pd.read_csv(\"data_files_USA/data_2015_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2016 = pd.read_csv(\"data_files_USA/data_2016_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [wos_2006, wos_2007, wos_2008, wos_2009, wos_2010, wos_2011, wos_2012, wos_2013, wos_2014, wos_2015, wos_2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_texts_cities = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_texts_orgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in wos.itertuples(index=True):\n",
    "    try:\n",
    "        text = remove_stopwords(strip_numeric(strip_non_alphanum(row.abstract.lower())))\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    if row.city not in research_texts_cities:\n",
    "        research_texts_cities[row.city] = []\n",
    "        research_texts_cities[row.city].append(text)\n",
    "    else:\n",
    "        research_texts_cities[row.city].append(text)\n",
    "\n",
    "    if row.org not in research_texts_orgs:\n",
    "        research_texts_orgs[row.org] = []\n",
    "        research_texts_orgs[row.org].append(text)\n",
    "    else:\n",
    "        research_texts_orgs[row.org].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_vectors_cities = {}\n",
    "research_vectors_orgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_vectors_cities = {}\n",
    "teaching_vectors_orgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, strip_numeric, strip_non_alphanum, stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in research_texts_cities:\n",
    "    if city not in research_vectors_cities and len(research_texts_cities[city]) > 0:\n",
    "        research_vectors_cities[city] = entity_vector(research_texts_cities[city], d2vmodel, \"doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for org in research_texts_orgs:\n",
    "    if org not in research_vectors_orgs and len(research_texts_orgs[org]) > 0:\n",
    "        research_vectors_orgs[org] = entity_vector(research_texts_orgs[org], d2vmodel, \"doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in teaching_texts_cities:\n",
    "    if city not in teaching_vectors_cities and len(teaching_texts_cities[city]) > 0:\n",
    "        teaching_vectors_cities[city] = entity_vector(teaching_texts_cities[city], d2vmodel, \"doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for org in teaching_texts_orgs:\n",
    "    if org not in teaching_vectors_orgs and len(teaching_texts_orgs[org]) > 0:\n",
    "        teaching_vectors_orgs[org] = entity_vector(teaching_texts_orgs[org], d2vmodel, \"doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('research_vectors_cities.txt', 'w') as file:\n",
    "    file.write(json.dumps(research_vectors_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('teaching_vectors_cities.txt', 'w') as file:\n",
    "    file.write(json.dumps(teaching_vectors_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('research_vectors_orgs.txt', 'w') as file:\n",
    "    file.write(json.dumps(research_vectors_orgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('teaching_vectors_orgs.txt', 'w') as file:\n",
    "    file.write(json.dumps(teaching_vectors_orgs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
