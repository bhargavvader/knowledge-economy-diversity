{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will clean and create the combined embedding space\n",
    "\n",
    "We will clean the teaching (syllabus), research (wos), and jobs (BG) spaces.\n",
    "\n",
    "First we'll open, get metadata, then create an embedding space.\n",
    "\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecoder, JSONDecodeError\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import TfidfModel\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import xml.etree.cElementTree as ET\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_WHITESPACE = re.compile(r'[^\\s]')\n",
    "def decode_stacked(document, pos=0, decoder=JSONDecoder()):\n",
    "    while True:\n",
    "        match = NOT_WHITESPACE.search(document, pos)\n",
    "        if not match:\n",
    "            return\n",
    "        pos = match.start()\n",
    "\n",
    "        try:\n",
    "            obj, pos = decoder.raw_decode(document, pos)\n",
    "        except JSONDecodeError:\n",
    "            # do something sensible if there's some error\n",
    "            raise\n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, strip_numeric, strip_non_alphanum, stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus, method=\"default\"):\n",
    "    cleaned = []\n",
    "    if method == \"default\":\n",
    "        for line in corpus:\n",
    "            cleaned.append(remove_stopwords(strip_numeric(strip_non_alphanum(line.lower()))))\n",
    "            \n",
    "    if method == \"basic\":\n",
    "        for line in corpus:\n",
    "            try:\n",
    "                cleaned.append(gensim.utils.simple_preprocess(line))\n",
    "            except TypeError:\n",
    "                continue\n",
    "        return cleaned\n",
    "    \n",
    "    if method == \"advanced\":\n",
    "        nlp = spacy.load(\"en\")\n",
    "        for abstract in corpus:\n",
    "            article = []\n",
    "            doc = nlp(abstract)\n",
    "            for w in doc:\n",
    "                # if it's not a stop word or punctuation mark, add it to our article!\n",
    "                if not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':\n",
    "                    # we add the lematized version of the word\n",
    "                    article.append(w.lemma_)\n",
    "            cleaned.append(article)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(corpus, method, **kwargs):\n",
    "    if method == \"doc2vec\":\n",
    "        try:\n",
    "            if kwargs[\"model_address\"]:\n",
    "                model = Doc2Vec.load(kwargs[\"model_address\"], binary=True)\n",
    "                return model\n",
    "        except KeyError:\n",
    "            pass\n",
    "        # vector size?\n",
    "        model = Doc2Vec(vector_size=100, min_count=5, epochs=40)\n",
    "        i = 0\n",
    "        train_corpus = []\n",
    "        for doc in corpus:\n",
    "            # process doc more?\n",
    "            train_corpus.append(TaggedDocument(doc, [i]))\n",
    "            i += 1\n",
    "        model.build_vocab(train_corpus)\n",
    "        model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        return model\n",
    "        \n",
    "    if method == \"word2vec\":\n",
    "        try:\n",
    "            if kwargs[\"model_address\"]:\n",
    "                model = KeyedVectors.load_word2vec_format(kwargs[\"model_address\"], binary=True)\n",
    "                return model\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            # for now, for allowing for our own parameters\n",
    "            model = Word2Vec(corpus, size=kwargs[\"size\"], hs=kwargs[\"hs\"])\n",
    "            return model\n",
    "        except KeyError:\n",
    "            # return basic word2vec model\n",
    "            model = Word2Vec(corpus, size=200, hs=1)\n",
    "            model = model.wv\n",
    "            return model\n",
    "        \n",
    "    if method == \"tfidf\":\n",
    "        \n",
    "        dct = Dictionary(corpus)\n",
    "        bow_corpus = [dct.doc2bow(line) for line in corpus]\n",
    "        model = TfidfModel(bow_corpus)\n",
    "        \n",
    "        low_value = 0.2\n",
    "        low_value_words = []\n",
    "        for bow in bow_corpus:\n",
    "            low_value_words += [id for id, value in model[bow] if value < low_value]\n",
    "        \n",
    "        dct.filter_tokens(bad_ids=low_value_words)\n",
    "        new_corpus = [dct.doc2bow(doc) for doc in corpus]\n",
    "            \n",
    "        return model, new_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "syllabus_text =[]\n",
    "syllabus_years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"openSyReal/\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(\"openSyReal/\" + filename, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                for jsonfile in decode_stacked(line):\n",
    "                        try:\n",
    "                            if jsonfile['grid_country_code'] == \"US\":\n",
    "                                try:\n",
    "                                    texts.append(jsonfile['text'])\n",
    "                                except KeyError:\n",
    "                                    continue\n",
    "                                try:\n",
    "                                    syllabus_years.append(jsonfile['year'])\n",
    "                                except KeyError:\n",
    "                                    syllabus_years.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_2006 = [i for i, j in enumerate(syllabus_years) if j != None and j >= 2006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabus_texts = list(np.array(texts)[indices_2006])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(\"/project2/jevans/BG/Text Data/\"):\n",
    "    if os.path.isdir(\"/project2/jevans/BG/Text Data/\" + folder):\n",
    "        for filename in os.listdir(\"/project2/jevans/BG/Text Data/\" + folder):\n",
    "            zfile = zipfile.ZipFile(\"/project2/jevans/BG/Text Data/\" + folder + \"/\" +filename)\n",
    "            jobs = ET.parse(zfile.open(zfile.infolist()[0])).getroot()\n",
    "            for job in jobs:\n",
    "                job_texts.append(job[7].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_2006 = pd.read_csv(\"data_files_USA/data_2006_US.csv\")\n",
    "wos_2007 = pd.read_csv(\"data_files_USA/data_2007_US.csv\")\n",
    "wos_2008 = pd.read_csv(\"data_files_USA/data_2008_US.csv\")\n",
    "wos_2009 = pd.read_csv(\"data_files_USA/data_2009_US.csv\")\n",
    "wos_2010 = pd.read_csv(\"data_files_USA/data_2010_US.csv\")\n",
    "wos_2011 = pd.read_csv(\"data_files_USA/data_2011_US.csv\")\n",
    "wos_2012 = pd.read_csv(\"data_files_USA/data_2012_US.csv\")\n",
    "wos_2013 = pd.read_csv(\"data_files_USA/data_2013_US.csv\")\n",
    "wos_2014 = pd.read_csv(\"data_files_USA/data_2014_US.csv\")\n",
    "wos_2015 = pd.read_csv(\"data_files_USA/data_2015_US.csv\")\n",
    "wos_2016 = pd.read_csv(\"data_files_USA/data_2016_US.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [wos_2006, wos_2007, wos_2008, wos_2009, wos_2010, wos_2011, wos_2012, wos_2013, wos_2014, wos_2015, wos_2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in wos.itertuples(index=True):\n",
    "    research_texts.append(row.abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jobs = clean_corpus(job_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del job_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_research = clean_corpus(research_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del research_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_teaching = clean_corpus(syllabus_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del syllabus_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = cleaned_teaching + cleaned_research + cleaned_jobs\n",
    "del cleaned_teaching\n",
    "del cleaned_research\n",
    "del cleaned_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(cleaned_corpus, method='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"alldoc2vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
